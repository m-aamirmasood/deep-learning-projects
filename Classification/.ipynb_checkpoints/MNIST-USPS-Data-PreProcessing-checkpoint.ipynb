{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss(w,x,y,lam):\n",
    "    m = x.shape[0]\n",
    "    y_enc = onehot_enc(y) #one-hot vector notation\n",
    "    a = np.dot(x,w)\n",
    "    prob = softmax(a)\n",
    "    loss = (-1 / m) * np.sum(y_enc * np.log(prob)) + (lam/2)*np.sum(w*w)\n",
    "    grad = (-1 / m) * np.dot(x.T,(y_enc - prob)) + lam*w\n",
    "    return loss,grad\n",
    "\n",
    "def onehot_enc(Y):\n",
    "    return (np.arange(np.max(Y) + 1) == Y[:, None]).astype(float)\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "def getProbsAndPreds(someX):\n",
    "    probs = softmax(np.dot(someX,w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "\n",
    "def getAccuracy(someX,someY):\n",
    "    prob,prede = getProbsAndPreds(someX)\n",
    "    accuracy = sum(prede == someY)/(float(len(someY)))\n",
    "    accuracy = accuracy*100\n",
    "    return accuracy\n",
    "\n",
    "def compute_confusion_matrix(true, pred):\n",
    "    prob,prede = getProbsAndPreds(pred)\n",
    "    K = len(np.unique(true)) # Number of classes \n",
    "    result = np.zeros((K, K))\n",
    "    for i in range(len(true)):\n",
    "        result[true[i]][prede[i]] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(50000, 784)\n",
      "(10000,)\n",
      "(10000, 784)\n",
      "(10000,)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "train_x, train_y = training_data\n",
    "TrainingTarget=np.array(train_y)\n",
    "print(TrainingTarget.shape)\n",
    "TrainingData=np.array(train_x)\n",
    "print(TrainingData.shape)\n",
    "validation_x, validation_y = validation_data\n",
    "ValidationTarget=np.array(validation_y)\n",
    "print(ValidationTarget.shape)\n",
    "ValidationData=np.array(validation_x)\n",
    "print(ValidationData.shape)\n",
    "test_x, test_y = test_data\n",
    "TestTarget=np.array(test_y)\n",
    "print(TestTarget.shape)\n",
    "TestData=np.array(test_x)\n",
    "print(TestData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19999, 784)\n",
      "(19999,)\n"
     ]
    }
   ],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)\n",
    "USPSTar= np.asarray(USPSTar)\n",
    "USPSMat= np.asarray(USPSMat)            \n",
    "print(USPSMat.shape)\n",
    "print(USPSTar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1461092304948473\n",
      "Training Accuracy:  71.616\n",
      "Test Accuracy:  72.52\n",
      "Test Accuracy:  24.34621731086554\n",
      "[[4.805e+03 0.000e+00 1.300e+01 2.200e+01 0.000e+00 0.000e+00 4.000e+01\n",
      "  0.000e+00 5.100e+01 1.000e+00]\n",
      " [6.000e+00 4.983e+03 3.900e+01 8.800e+01 1.000e+00 0.000e+00 1.600e+01\n",
      "  3.000e+00 5.370e+02 5.000e+00]\n",
      " [5.030e+02 6.000e+01 3.580e+03 2.780e+02 2.000e+01 0.000e+00 2.300e+02\n",
      "  4.500e+01 2.350e+02 1.700e+01]\n",
      " [3.270e+02 2.100e+01 1.200e+02 4.209e+03 1.000e+00 0.000e+00 4.300e+01\n",
      "  3.500e+01 3.110e+02 3.400e+01]\n",
      " [3.110e+02 3.000e+01 6.200e+01 4.700e+01 2.636e+03 0.000e+00 2.340e+02\n",
      "  1.800e+01 4.050e+02 1.116e+03]\n",
      " [1.825e+03 7.000e+01 1.060e+02 1.183e+03 1.400e+01 0.000e+00 1.420e+02\n",
      "  2.200e+01 1.034e+03 1.100e+02]\n",
      " [5.530e+02 4.500e+01 1.020e+02 2.200e+01 3.000e+00 0.000e+00 4.131e+03\n",
      "  0.000e+00 9.400e+01 1.000e+00]\n",
      " [3.320e+02 1.180e+02 9.000e+01 5.200e+01 1.800e+01 0.000e+00 1.600e+01\n",
      "  4.098e+03 2.330e+02 2.180e+02]\n",
      " [2.750e+02 5.000e+01 5.200e+01 5.920e+02 1.000e+00 0.000e+00 4.700e+01\n",
      "  8.000e+00 3.757e+03 6.000e+01]\n",
      " [3.520e+02 3.900e+01 4.600e+01 1.550e+02 7.600e+01 0.000e+00 8.000e+00\n",
      "  2.270e+02 4.760e+02 3.609e+03]]\n",
      "[[1.388e+03 2.000e+00 2.600e+02 5.400e+01 1.080e+02 0.000e+00 2.400e+01\n",
      "  1.200e+01 5.700e+01 9.500e+01]\n",
      " [4.820e+02 1.980e+02 2.360e+02 2.500e+02 4.200e+01 0.000e+00 3.300e+01\n",
      "  3.370e+02 4.130e+02 9.000e+00]\n",
      " [8.010e+02 1.800e+01 8.320e+02 9.500e+01 1.100e+01 0.000e+00 7.900e+01\n",
      "  4.700e+01 1.120e+02 4.000e+00]\n",
      " [7.980e+02 1.000e+00 7.700e+01 8.900e+02 8.000e+00 0.000e+00 2.500e+01\n",
      "  2.800e+01 1.430e+02 3.000e+01]\n",
      " [6.170e+02 5.700e+01 6.900e+01 1.210e+02 5.190e+02 0.000e+00 3.000e+01\n",
      "  1.420e+02 3.800e+02 6.500e+01]\n",
      " [1.128e+03 1.200e+01 1.720e+02 3.530e+02 6.000e+00 0.000e+00 7.400e+01\n",
      "  2.900e+01 2.080e+02 1.800e+01]\n",
      " [1.321e+03 5.000e+00 2.150e+02 5.800e+01 3.600e+01 0.000e+00 2.990e+02\n",
      "  5.000e+00 5.500e+01 6.000e+00]\n",
      " [4.540e+02 1.810e+02 3.990e+02 3.600e+02 8.000e+00 0.000e+00 4.800e+01\n",
      "  1.880e+02 3.520e+02 1.000e+01]\n",
      " [9.280e+02 2.000e+01 1.910e+02 2.110e+02 2.800e+01 0.000e+00 1.090e+02\n",
      "  1.800e+01 4.770e+02 1.800e+01]\n",
      " [3.930e+02 1.540e+02 1.770e+02 4.890e+02 3.300e+01 0.000e+00 1.300e+01\n",
      "  2.410e+02 4.220e+02 7.800e+01]]\n"
     ]
    }
   ],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "num_classes = 10\n",
    "\n",
    "w = np.zeros([TrainingData.shape[1],len(np.unique(TrainingTarget))])\n",
    "lam = 1\n",
    "iterations = 20\n",
    "learningRate = 0.01\n",
    "losses = []\n",
    "for i in range(0,iterations):\n",
    "    loss,grad = getLoss(w,TrainingData,TrainingTarget,lam)\n",
    "    losses.append(loss)\n",
    "    w = w - (learningRate * grad)\n",
    "print (loss)\n",
    "print ('Training Accuracy: ', getAccuracy(TrainingData,TrainingTarget))\n",
    "print ('Test Accuracy: ', getAccuracy(TestData,TestTarget))\n",
    "print ('Test Accuracy: ', getAccuracy(USPSMat,USPSTar))\n",
    "cm1 = compute_confusion_matrix(TrainingTarget, TrainingData)\n",
    "print(cm1)\n",
    "cm2= compute_confusion_matrix(USPSTar, USPSMat)\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000, 10)\n",
      "0.9376\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_14 to have 2 dimensions, but got array with shape (19999, 10, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-1a4227b3fcc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSPSMat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mUSPSTar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#cm3= compute_confusion_matrix(y_test, x_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_14 to have 2 dimensions, but got array with shape (19999, 10, 10)"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "num_classes=10\n",
    "image_vector_size=28*28\n",
    "x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n",
    "USPSMat = USPSMat.reshape(USPSMat.shape[0], image_vector_size)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "USPSTar = keras.utils.to_categorical(USPSTar, num_classes)\n",
    "image_size = 784 \n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='sigmoid', input_shape=(image_size,)))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=100,\n",
    "verbose=False,validation_split=.1)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "loss,accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(accuracy)\n",
    "loss,accuracy = model.evaluate(USPSMat,USPSTar, verbose=False)\n",
    "print(accuracy)\n",
    "cm3= compute_confusion_matrix(y_test, x_test)\n",
    "print(cm3)\n",
    "cm4= compute_confusion_matrix(USPSTar, USPSMat)\n",
    "print(cm4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/Documents/anaconda/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# SVM & RandomForest\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "n_train = 60000\n",
    "n_test = 10000\n",
    "indices = np.arange(len(mnist.data))\n",
    "train_idx = np.arange(0,n_train)\n",
    "test_idx = np.arange(n_train+1,n_train+n_test)\n",
    "X_train, y_train = mnist.data[train_idx], mnist.target[train_idx]\n",
    "X_test, y_test = mnist.data[test_idx], mnist.target[test_idx]\n",
    "# SVM\n",
    "classifier1 = SVC(kernel='rbf', C=2, gamma = 0.05);\n",
    "classifier1.fit(X_train, y_train)\n",
    "classifier1.score(X_train,y_train)\n",
    "cm5= compute_confusion_matrix(USPSTar, USPSMat)\n",
    "print(cm5)\n",
    "classifier1.fit(USPSMat, USPSTar)\n",
    "classifier1.score(USPSMat, USPSTar)\n",
    "cm6= compute_confusion_matrix(USPSTar, USPSMat)\n",
    "print(cm6)\n",
    "#RandomForestClassifier\n",
    "classifier2 = RandomForestClassifier(n_estimator=10);\n",
    "classifier2.fit(X_train, y_train) \n",
    "classifier2.score(X_train,y_train)\n",
    "cm7= compute_confusion_matrix(USPSTar, USPSMat)\n",
    "print(cm7)\n",
    "classifier2.fit(USPSMat, USPSTar)\n",
    "classifier2.score(USPSMat, USPSTar)\n",
    "cm8= compute_confusion_matrix(USPSTar, USPSMat)\n",
    "print(cm8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "estimators.append(('svm', classifier1))\n",
    "estimators.append(('random', classifier2))\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, TrainingData, TrainingTarget, cv=kfold)\n",
    "print(results.mean())\n",
    "results = model_selection.cross_val_score(ensemble, USPSMat, USPSTar, cv=kfold)\n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
